{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tcn in /opt/conda/lib/python3.7/site-packages (3.1.1)\r\n",
      "Requirement already satisfied: keras==2.3.1 in /opt/conda/lib/python3.7/site-packages (from keras-tcn) (2.3.1)\r\n",
      "Requirement already satisfied: numpy>=1.18.1 in /opt/conda/lib/python3.7/site-packages (from keras-tcn) (1.18.5)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (1.0.8)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (5.3.1)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (1.1.2)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (1.4.1)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (1.15.0)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (2.10.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = './content/coinbase_btc_usd/coinbase/btc_usd/l2_snapshots/100ms/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_data(base_path):\n",
    "    \"\"\"Concatenate all the files in basepath keeping only the\n",
    "    columns specified by features.\n",
    "    \"\"\"\n",
    "    l2_snapshot = pd.DataFrame()\n",
    "    for i, x in enumerate(tqdm(sorted(os.listdir(base_path)))):\n",
    "        if base_path is None:\n",
    "            path = x\n",
    "        else:\n",
    "            path = base_path + x\n",
    "        df_hour = pd.read_parquet(path)\n",
    "        l2_snapshot = pd.concat([l2_snapshot, df_hour.dropna()])\n",
    "        \n",
    "    return l2_snapshot\n",
    "\n",
    "def extend_matrix(A, n):\n",
    "    \"\"\"Extend a matrix A by duplicating rows as specified by the list n.\"\"\"\n",
    "    n = n[1:]  # Do not duplicate rows for the first day\n",
    "    A = A[:-1]  # Do not duplicate the last day's row\n",
    "    A = np.repeat(A, repeats=n, axis=0)\n",
    "    return A\n",
    "\n",
    "def normalise_data_per_day(df):\n",
    "    df_mean = df.resample('D').mean()\n",
    "    df_var = df.resample('D').var()\n",
    "    \n",
    "    timestamps_per_day = np.unique(df.index.date, return_counts=True)[1]\n",
    "    mean_array = extend_matrix(df_mean.to_numpy(), timestamps_per_day)\n",
    "    var_array = extend_matrix(df_var.to_numpy(), timestamps_per_day)\n",
    "    \n",
    "    # Drop the rows of the first day\n",
    "    df = df[df.index.date != df.index[0].date()]\n",
    "    \n",
    "    df = (df - mean_array) / np.sqrt(var_array)\n",
    "\n",
    "    return df\n",
    "\n",
    "def balance_classes(y):\n",
    "    unique = np.unique(y, return_counts=True)\n",
    "\n",
    "    # Take smallest number as class size\n",
    "    class_size = np.min(unique[1])\n",
    "    class_size_index = np.argmin(unique[1])\n",
    "    timestamps = np.array([], dtype=int)\n",
    "    for i, category in enumerate(unique[0]):\n",
    "        if i == class_size_index:\n",
    "            continue\n",
    "        index = np.argwhere(y==category)\n",
    "        index = index.reshape(len(index))\n",
    "        random_timestamps = np.random.choice(index, (unique[1][i] - class_size), replace=False)\n",
    "        timestamps = np.concatenate((timestamps, random_timestamps), axis=None)\n",
    "        \n",
    "    return timestamps\n",
    "\n",
    "def generate_y(df_snapshot, k, T=100, D=40, best_ask='a1', best_bid='b1', alpha=10e-5):\n",
    "    \"\"\"Return X, y from the snapshot dataframe and the best ask/bid columns.\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['mid_price'] = (df_snapshot[best_ask].to_numpy()+df_snapshot[best_bid].to_numpy())/2\n",
    "\n",
    "    # Create columns delayed by -k to k-1\n",
    "    for i in range(-k, k):\n",
    "        df[i] = df['mid_price'].shift(periods=i)\n",
    "\n",
    "    # Drop first k-1 rows and last k rows\n",
    "    df.drop(range(0,k), axis=0, inplace=True)\n",
    "    df.drop(range(len(df_snapshot)-k,len(df_snapshot)), axis=0, inplace=True)\n",
    "    \n",
    "    # Compute mean of previous k and next k\n",
    "    df['m_b'] = df[range(0,k)].mean(axis=1)\n",
    "    df['m_a'] = df[range(-k,0)].mean(axis=1)\n",
    "    \n",
    "    # Compute label of increasing or decreasing\n",
    "    y_increase = np.where(df['m_b'] > df['m_a'] * (1+alpha), 1, 0)\n",
    "    y_decrease = np.where(df['m_b'] < df['m_a'] * (1-alpha), -1, 0)\n",
    "    y = y_increase + y_decrease\n",
    "\n",
    "    # 100 most recent limit orders used so ignore first 100 timesteps\n",
    "    y = y[T:]\n",
    "    y += 1\n",
    "\n",
    "    return y\n",
    "\n",
    "def generate_preX(df_snapshot, k):\n",
    "    preX = df_snapshot.to_numpy()[k:-k]\n",
    "\n",
    "    return preX\n",
    "\n",
    "def generate_X(preX, T=100, D=40):\n",
    "    # For each timestep create matrix of 100 most recent limit orders\n",
    "    X = np.array([preX[t:t+T] for t in range(len(preX)-T)], dtype='float32')\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [08:03<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# l2_snap = pd.read_csv('total.csv', index_col=0, infer_datetime_format=True)\n",
    "l2_snap = concat_data(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>...</th>\n",
       "      <th>aq41</th>\n",
       "      <th>aq42</th>\n",
       "      <th>aq43</th>\n",
       "      <th>aq44</th>\n",
       "      <th>aq45</th>\n",
       "      <th>aq46</th>\n",
       "      <th>aq47</th>\n",
       "      <th>aq48</th>\n",
       "      <th>aq49</th>\n",
       "      <th>aq50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.000</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.55</td>\n",
       "      <td>8719.50</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>8717.56</td>\n",
       "      <td>8716.06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>0.933419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.100</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.56</td>\n",
       "      <td>8719.55</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>8717.56</td>\n",
       "      <td>8716.06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>0.933419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.200</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.61</td>\n",
       "      <td>8719.56</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>8716.06</td>\n",
       "      <td>8716.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>8.200</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.300</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.61</td>\n",
       "      <td>8719.56</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8718.00</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>8716.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>8.200</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.400</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.61</td>\n",
       "      <td>8719.56</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.99</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8718.00</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>8.200</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              b1       b2       b3       b4      b5       b6  \\\n",
       "timestamp                                                                      \n",
       "2019-11-12 00:00:00.000  8721.53  8720.59  8719.55  8719.50  8719.0  8718.02   \n",
       "2019-11-12 00:00:00.100  8721.53  8720.59  8719.56  8719.55  8719.0  8718.02   \n",
       "2019-11-12 00:00:00.200  8721.53  8720.59  8719.61  8719.56  8719.0  8718.02   \n",
       "2019-11-12 00:00:00.300  8721.53  8720.59  8719.61  8719.56  8719.0  8718.02   \n",
       "2019-11-12 00:00:00.400  8721.53  8720.59  8719.61  8719.56  8719.0  8718.99   \n",
       "\n",
       "                              b7       b8       b9      b10  ...      aq41  \\\n",
       "timestamp                                                    ...             \n",
       "2019-11-12 00:00:00.000  8717.87  8717.85  8717.56  8716.06  ...  8.200000   \n",
       "2019-11-12 00:00:00.100  8717.87  8717.85  8717.56  8716.06  ...  8.200000   \n",
       "2019-11-12 00:00:00.200  8717.87  8717.85  8716.06  8716.00  ...  0.009135   \n",
       "2019-11-12 00:00:00.300  8718.00  8717.87  8717.85  8716.60  ...  0.009135   \n",
       "2019-11-12 00:00:00.400  8718.02  8718.00  8717.87  8717.85  ...  0.009135   \n",
       "\n",
       "                          aq42   aq43   aq44   aq45      aq46      aq47  \\\n",
       "timestamp                                                                 \n",
       "2019-11-12 00:00:00.000  2.203  2.600  0.005  2.500  0.896321  0.766000   \n",
       "2019-11-12 00:00:00.100  2.203  2.600  0.005  2.500  0.896321  0.766000   \n",
       "2019-11-12 00:00:00.200  8.200  2.203  2.600  0.005  2.500000  0.896321   \n",
       "2019-11-12 00:00:00.300  8.200  2.203  2.600  0.005  2.500000  0.896321   \n",
       "2019-11-12 00:00:00.400  8.200  2.203  2.600  0.005  2.500000  0.896321   \n",
       "\n",
       "                             aq48      aq49      aq50  \n",
       "timestamp                                              \n",
       "2019-11-12 00:00:00.000  0.001737  1.820000  0.933419  \n",
       "2019-11-12 00:00:00.100  0.001737  1.820000  0.933419  \n",
       "2019-11-12 00:00:00.200  0.766000  0.001737  1.820000  \n",
       "2019-11-12 00:00:00.300  0.766000  0.001737  1.820000  \n",
       "2019-11-12 00:00:00.400  0.766000  0.001737  1.820000  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_snap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>...</th>\n",
       "      <th>aq41</th>\n",
       "      <th>aq42</th>\n",
       "      <th>aq43</th>\n",
       "      <th>aq44</th>\n",
       "      <th>aq45</th>\n",
       "      <th>aq46</th>\n",
       "      <th>aq47</th>\n",
       "      <th>aq48</th>\n",
       "      <th>aq49</th>\n",
       "      <th>aq50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:57.900</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:58.400</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:58.500</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:58.800</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:59.600</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              b1       b2       b3       b4       b5       b6  \\\n",
       "timestamp                                                                       \n",
       "2019-11-20 23:59:57.900  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "2019-11-20 23:59:58.400  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "2019-11-20 23:59:58.500  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "2019-11-20 23:59:58.800  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "2019-11-20 23:59:59.600  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "\n",
       "                              b7       b8      b9      b10  ...   aq41  \\\n",
       "timestamp                                                   ...          \n",
       "2019-11-20 23:59:57.900  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "2019-11-20 23:59:58.400  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "2019-11-20 23:59:58.500  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "2019-11-20 23:59:58.800  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "2019-11-20 23:59:59.600  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "\n",
       "                             aq42    aq43   aq44  aq45      aq46   aq47  \\\n",
       "timestamp                                                                 \n",
       "2019-11-20 23:59:57.900  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "2019-11-20 23:59:58.400  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "2019-11-20 23:59:58.500  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "2019-11-20 23:59:58.800  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "2019-11-20 23:59:59.600  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "\n",
       "                          aq48  aq49     aq50  \n",
       "timestamp                                      \n",
       "2019-11-20 23:59:57.900  0.001   2.4  0.74149  \n",
       "2019-11-20 23:59:58.400  0.001   2.4  0.74149  \n",
       "2019-11-20 23:59:58.500  0.001   2.4  0.74149  \n",
       "2019-11-20 23:59:58.800  0.001   2.4  0.74149  \n",
       "2019-11-20 23:59:59.600  0.001   2.4  0.74149  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_snap.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Model and Depth Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(depth):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "\n",
    "        i = Input(batch_shape=(None, 100, depth))\n",
    "        o = TCN(return_sequences=False, use_skip_connections=True, dropout_rate=0.4, dilations=[1, 2, 4, 8, 16, 32, 64], use_batch_norm=True)(i)\n",
    "        o = Dense(3, activation='softmax')(o)\n",
    "        m = Model(inputs=[i], outputs=[o])\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "        m.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(l2_snap, depth, columns, days_training, k=20):\n",
    "    l2_snap.index = pd.to_datetime(l2_snap.index)\n",
    "    l2_snap_dep2 = l2_snap.loc[:,columns]\n",
    "    l2_norm = normalise_data_per_day(l2_snap_dep2)\n",
    "    T = 100\n",
    "    D = depth\n",
    "    y = generate_y(l2_norm, T=T, D=D, best_ask='a1', best_bid = 'b1', alpha=0.002, k=k)\n",
    "    preX = generate_preX(l2_norm, k=k)\n",
    "    print(\"preX Shape: \", preX.shape)\n",
    "    X = generate_X(preX)\n",
    "    print(\"X shape, y shape: \", X.shape, y.shape)\n",
    "    print(\"Unique y's: \", np.unique(y))\n",
    "\n",
    "    del preX\n",
    "    gc.collect()\n",
    "\n",
    "    # First and last 20 are removed to create labels and then last 100 as previous 100 is required for input matrix\n",
    "    X_index = l2_norm[20:-120].index\n",
    "\n",
    "    # Number of data points for the last day\n",
    "    num_test = np.unique(X_index.day, return_counts=True)[1][-1]\n",
    "\n",
    "    # Days back to\n",
    "    num_start = np.unique(X_index.day, return_counts=True)[1][-1 - days_training:].sum()\n",
    "\n",
    "    # Split the data into the first days and the last day\n",
    "    X_train_val = X[-num_start:-num_test]\n",
    "    y_train_val = y[-num_start:-num_test]\n",
    "    X_test = X[-num_test:]\n",
    "    y_test = y[-num_test:]\n",
    "    return X_train_val, y_train_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth:10\n",
    "\n",
    "\n",
    "#### Days 1, 2, 4, 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days Training now  1\n",
      "Level : 1\n",
      "preX Shape:  (4220032, 4)\n",
      "X shape, y shape:  (4219932, 100, 4) (4219932,)\n",
      "Unique y's:  [0 1 2]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[0 1 2], y=[1 1 1 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.7580 - accuracy: 0.4680\n",
      "Epoch 00001: val_loss improved from inf to 1.16982, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 122s 30ms/step - loss: 0.7580 - accuracy: 0.4680 - val_loss: 1.1698 - val_accuracy: 0.3753 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.7097 - accuracy: 0.5074\n",
      "Epoch 00002: val_loss did not improve from 1.16982\n",
      "4128/4128 [==============================] - 116s 28ms/step - loss: 0.7097 - accuracy: 0.5073 - val_loss: 1.5694 - val_accuracy: 0.3366 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6963 - accuracy: 0.5241\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.16982\n",
      "4128/4128 [==============================] - 117s 28ms/step - loss: 0.6963 - accuracy: 0.5241 - val_loss: 1.2914 - val_accuracy: 0.3985 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6730 - accuracy: 0.5450\n",
      "Epoch 00004: val_loss did not improve from 1.16982\n",
      "4128/4128 [==============================] - 117s 28ms/step - loss: 0.6730 - accuracy: 0.5450 - val_loss: 1.4283 - val_accuracy: 0.4211 - lr: 0.0050\n",
      "Epoch 5/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6663 - accuracy: 0.5524\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.16982\n",
      "4128/4128 [==============================] - 117s 28ms/step - loss: 0.6663 - accuracy: 0.5524 - val_loss: 1.5630 - val_accuracy: 0.3940 - lr: 0.0050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.26      0.65      0.38     61142\n",
      "      Stable       0.91      0.28      0.43    350499\n",
      "          Up       0.22      0.85      0.35     53759\n",
      "\n",
      "    accuracy                           0.39    465400\n",
      "   macro avg       0.46      0.59      0.38    465400\n",
      "weighted avg       0.74      0.39      0.41    465400\n",
      "\n",
      "ACCURACY:  0.3940244950580146\n",
      "Days Training now  1\n",
      "Level : 2\n",
      "preX Shape:  (4220032, 8)\n",
      "X shape, y shape:  (4219932, 100, 8) (4219932,)\n",
      "Unique y's:  [0 1 2]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[0 1 2], y=[1 1 1 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.7441 - accuracy: 0.4893\n",
      "Epoch 00001: val_loss improved from inf to 1.22091, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 124s 30ms/step - loss: 0.7441 - accuracy: 0.4893 - val_loss: 1.2209 - val_accuracy: 0.3478 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6953 - accuracy: 0.5390\n",
      "Epoch 00002: val_loss did not improve from 1.22091\n",
      "4128/4128 [==============================] - 121s 29ms/step - loss: 0.6953 - accuracy: 0.5390 - val_loss: 1.3443 - val_accuracy: 0.3660 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6810 - accuracy: 0.5495\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.22091\n",
      "4128/4128 [==============================] - 122s 29ms/step - loss: 0.6809 - accuracy: 0.5495 - val_loss: 1.5957 - val_accuracy: 0.3902 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6540 - accuracy: 0.5677\n",
      "Epoch 00004: val_loss did not improve from 1.22091\n",
      "4128/4128 [==============================] - 121s 29ms/step - loss: 0.6540 - accuracy: 0.5676 - val_loss: 1.2788 - val_accuracy: 0.4481 - lr: 0.0050\n",
      "Epoch 5/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6462 - accuracy: 0.5721\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.22091\n",
      "4128/4128 [==============================] - 121s 29ms/step - loss: 0.6462 - accuracy: 0.5721 - val_loss: 1.3845 - val_accuracy: 0.4411 - lr: 0.0050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.28      0.60      0.38     61142\n",
      "      Stable       0.85      0.36      0.51    350499\n",
      "          Up       0.23      0.77      0.35     53759\n",
      "\n",
      "    accuracy                           0.44    465400\n",
      "   macro avg       0.45      0.58      0.41    465400\n",
      "weighted avg       0.70      0.44      0.47    465400\n",
      "\n",
      "ACCURACY:  0.4410528577567684\n",
      "Days Training now  1\n",
      "Level : 5\n",
      "preX Shape:  (4220032, 20)\n",
      "X shape, y shape:  (4219932, 100, 20) (4219932,)\n",
      "Unique y's:  [0 1 2]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.7529 - accuracy: 0.4934\n",
      "Epoch 00001: val_loss improved from inf to 1.05243, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 134s 32ms/step - loss: 0.7529 - accuracy: 0.4934 - val_loss: 1.0524 - val_accuracy: 0.3939 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.5549\n",
      "Epoch 00002: val_loss did not improve from 1.05243\n",
      "4128/4128 [==============================] - 128s 31ms/step - loss: 0.6841 - accuracy: 0.5549 - val_loss: 1.4293 - val_accuracy: 0.3483 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6660 - accuracy: 0.5698\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.05243\n",
      "4128/4128 [==============================] - 126s 31ms/step - loss: 0.6660 - accuracy: 0.5697 - val_loss: 1.2108 - val_accuracy: 0.4949 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.5849\n",
      "Epoch 00004: val_loss did not improve from 1.05243\n",
      "4128/4128 [==============================] - 127s 31ms/step - loss: 0.6339 - accuracy: 0.5849 - val_loss: 1.5714 - val_accuracy: 0.4554 - lr: 0.0050\n",
      "Epoch 5/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6222 - accuracy: 0.5935\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.05243\n",
      "4128/4128 [==============================] - 125s 30ms/step - loss: 0.6221 - accuracy: 0.5935 - val_loss: 1.6850 - val_accuracy: 0.5248 - lr: 0.0050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.33      0.38      0.35     61142\n",
      "      Stable       0.82      0.52      0.64    350499\n",
      "          Up       0.22      0.70      0.34     53759\n",
      "\n",
      "    accuracy                           0.52    465400\n",
      "   macro avg       0.46      0.53      0.44    465400\n",
      "weighted avg       0.68      0.52      0.57    465400\n",
      "\n",
      "ACCURACY:  0.5247657928663515\n",
      "Days Training now  1\n",
      "Level : 10\n",
      "preX Shape:  (4220032, 40)\n",
      "X shape, y shape:  (4219932, 100, 40) (4219932,)\n",
      "Unique y's:  [0 1 2]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.7557 - accuracy: 0.4855\n",
      "Epoch 00001: val_loss improved from inf to 1.65787, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 145s 35ms/step - loss: 0.7557 - accuracy: 0.4855 - val_loss: 1.6579 - val_accuracy: 0.3115 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6829 - accuracy: 0.5580\n",
      "Epoch 00002: val_loss improved from 1.65787 to 1.60203, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 137s 33ms/step - loss: 0.6829 - accuracy: 0.5580 - val_loss: 1.6020 - val_accuracy: 0.5047 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.5743\n",
      "Epoch 00003: val_loss did not improve from 1.60203\n",
      "4128/4128 [==============================] - 137s 33ms/step - loss: 0.6668 - accuracy: 0.5743 - val_loss: 1.6513 - val_accuracy: 0.4988 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6535 - accuracy: 0.5846\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.60203\n",
      "4128/4128 [==============================] - 137s 33ms/step - loss: 0.6535 - accuracy: 0.5846 - val_loss: 2.2778 - val_accuracy: 0.4420 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6272 - accuracy: 0.5957\n",
      "Epoch 00005: val_loss did not improve from 1.60203\n",
      "4128/4128 [==============================] - 137s 33ms/step - loss: 0.6271 - accuracy: 0.5957 - val_loss: 2.7081 - val_accuracy: 0.4912 - lr: 0.0050\n",
      "Epoch 6/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6202 - accuracy: 0.6017\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.60203\n",
      "4128/4128 [==============================] - 136s 33ms/step - loss: 0.6202 - accuracy: 0.6017 - val_loss: 2.7999 - val_accuracy: 0.4699 - lr: 0.0050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.26      0.36      0.30     61142\n",
      "      Stable       0.78      0.46      0.58    350499\n",
      "          Up       0.20      0.63      0.30     53759\n",
      "\n",
      "    accuracy                           0.47    465400\n",
      "   macro avg       0.41      0.49      0.40    465400\n",
      "weighted avg       0.65      0.47      0.51    465400\n",
      "\n",
      "ACCURACY:  0.4698345509239364\n",
      "Days Training now  1\n",
      "Level : 20\n",
      "preX Shape:  (4220032, 80)\n",
      "X shape, y shape:  (4219932, 100, 80) (4219932,)\n",
      "Unique y's:  [0 1 2]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.7669 - accuracy: 0.4788\n",
      "Epoch 00001: val_loss improved from inf to 1.44405, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 164s 40ms/step - loss: 0.7669 - accuracy: 0.4788 - val_loss: 1.4441 - val_accuracy: 0.2838 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6857 - accuracy: 0.5574\n",
      "Epoch 00002: val_loss did not improve from 1.44405\n",
      "4128/4128 [==============================] - 160s 39ms/step - loss: 0.6856 - accuracy: 0.5574 - val_loss: 1.6881 - val_accuracy: 0.3432 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.5727\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.44405\n",
      "4128/4128 [==============================] - 163s 39ms/step - loss: 0.6696 - accuracy: 0.5727 - val_loss: 3.3865 - val_accuracy: 0.4089 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6400 - accuracy: 0.5936\n",
      "Epoch 00004: val_loss did not improve from 1.44405\n",
      "4128/4128 [==============================] - 162s 39ms/step - loss: 0.6400 - accuracy: 0.5936 - val_loss: 3.7938 - val_accuracy: 0.3822 - lr: 0.0050\n",
      "Epoch 5/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6293 - accuracy: 0.6018\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.44405\n",
      "4128/4128 [==============================] - 158s 38ms/step - loss: 0.6294 - accuracy: 0.6018 - val_loss: 3.6793 - val_accuracy: 0.3849 - lr: 0.0050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.25      0.45      0.32     61142\n",
      "      Stable       0.77      0.32      0.46    350499\n",
      "          Up       0.18      0.71      0.29     53759\n",
      "\n",
      "    accuracy                           0.38    465400\n",
      "   macro avg       0.40      0.50      0.36    465400\n",
      "weighted avg       0.63      0.38      0.42    465400\n",
      "\n",
      "ACCURACY:  0.3848345509239364\n",
      "Days Training now  1\n",
      "Level : 40\n",
      "preX Shape:  (4220032, 160)\n",
      "X shape, y shape:  (4219932, 100, 160) (4219932,)\n",
      "Unique y's:  [0 1 2]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Epoch 1/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.7736 - accuracy: 0.4617\n",
      "Epoch 00001: val_loss improved from inf to 1.59029, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 215s 52ms/step - loss: 0.7736 - accuracy: 0.4617 - val_loss: 1.5903 - val_accuracy: 0.3358 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6904 - accuracy: 0.5454\n",
      "Epoch 00002: val_loss did not improve from 1.59029\n",
      "4128/4128 [==============================] - 208s 50ms/step - loss: 0.6904 - accuracy: 0.5454 - val_loss: 2.7704 - val_accuracy: 0.5522 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6736 - accuracy: 0.5619\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.59029\n",
      "4128/4128 [==============================] - 211s 51ms/step - loss: 0.6736 - accuracy: 0.5619 - val_loss: 8.0973 - val_accuracy: 0.4863 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6435 - accuracy: 0.5806\n",
      "Epoch 00004: val_loss did not improve from 1.59029\n",
      "4128/4128 [==============================] - 208s 50ms/step - loss: 0.6435 - accuracy: 0.5806 - val_loss: 6.8325 - val_accuracy: 0.4150 - lr: 0.0050\n",
      "Epoch 5/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6319 - accuracy: 0.5883\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.59029\n",
      "4128/4128 [==============================] - 204s 49ms/step - loss: 0.6319 - accuracy: 0.5883 - val_loss: 8.3183 - val_accuracy: 0.4516 - lr: 0.0050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.24      0.43      0.31     61142\n",
      "      Stable       0.77      0.44      0.56    350499\n",
      "          Up       0.19      0.54      0.28     53759\n",
      "\n",
      "    accuracy                           0.45    465400\n",
      "   macro avg       0.40      0.47      0.38    465400\n",
      "weighted avg       0.63      0.45      0.49    465400\n",
      "\n",
      "ACCURACY:  0.4517425870219166\n",
      "Days Training now  1\n",
      "Level : 50\n",
      "preX Shape:  (4220032, 200)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 314. GiB for an array with shape (4219932, 100, 200) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fcd825db94b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Level :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mX_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_snap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdays_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdays_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3d41f67b8b37>\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m(l2_snap, depth, columns, days_training, k)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_preX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preX Shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X shape, y shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unique y's: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-af383498956e>\u001b[0m in \u001b[0;36mgenerate_X\u001b[0;34m(preX, T, D)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# For each timestep create matrix of 100 most recent limit orders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 314. GiB for an array with shape (4219932, 100, 200) and data type float32"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 7 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "days_training = 1\n",
    "k = 20\n",
    "levels = {}\n",
    "for level in [1, 2, 5, 10, 20, 40, 50]:\n",
    "    columns = []\n",
    "    for i in range(level):\n",
    "        j = i + 1\n",
    "        columns.extend(['b{}'.format(j), 'bq{}'.format(j), 'a{}'.format(j), 'aq{}'.format(j)])\n",
    "    print('Days Training now ', days_training)\n",
    "    print('Level :', level)\n",
    "    depth = level * 4\n",
    "    X_train_val, y_train_val, X_test, y_test = generate_data(l2_snap, depth=depth, columns = columns, days_training=days_training, k=k)\n",
    "    m = generate_model(depth=depth)\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    class_weights = compute_class_weight('balanced', np.unique(y_train_val), y_train_val)\n",
    "    d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    EPOCHS = 20\n",
    "    model_filename = \"model_resnet_walkforward_tcn_depth10.h5\"\n",
    "    hist_filename = \"hist_model_walkforward_tcn_depth10.csv\"\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=4)\n",
    "    model_save_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "                                filepath= model_filename,\n",
    "                                save_best_only=True,\n",
    "                                monitor='val_loss',\n",
    "                                verbose=1)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=2,\n",
    "                    verbose=1,\n",
    "                    min_lr=0.0001)\n",
    "    callbacks = [reduce_lr, model_save_checkpoint, early_stop]\n",
    "    history = m.fit(\n",
    "                X_train_val, y_train_val,\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=128,\n",
    "                callbacks=callbacks,\n",
    "                class_weight = d_class_weights,\n",
    "                validation_data = (X_test, y_test),\n",
    "    )\n",
    "\n",
    "    y_pred = m.predict(X_test).argmax(axis=1)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    sns.set()\n",
    "    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    ax=sns.heatmap(cm, annot=True, xticklabels=['Down','Stable','Up'], yticklabels=['Down','Stable','Up'], cmap='Blues')\n",
    "    ax.set_ylim(3.0, 0)\n",
    "    plt.savefig('image_depth_10_{}_days_training'.format(days_training))\n",
    "\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    print(classification_report(y_true = y_test, y_pred = y_pred,  target_names=['Down', 'Stable', 'Up']))\n",
    "    accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\n",
    "    print('ACCURACY: ', accuracy)\n",
    "    levels[depth] = accuracy\n",
    "    del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 0.3940244950580146,\n",
       " 8: 0.4410528577567684,\n",
       " 20: 0.5247657928663515,\n",
       " 40: 0.4698345509239364,\n",
       " 80: 0.3848345509239364,\n",
       " 160: 0.4517425870219166}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list(np.array(list(levels.keys()))/4)\n",
    "plt.plot(list(np.array(list(levels.keys()))/4), list(levels.values()))\n",
    "plt.ylabel('Walk-forward accuracy')\n",
    "plt.xlabel('Time Horizon s')\n",
    "plt.ylim([0, 0.7])\n",
    "# plt.savefig('walkforward_days_training')\n",
    "list(np.array(list(levels.keys()))/4)\n",
    "levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_val, y_train_val, X_test, y_test = generate_data(l2_snap, 20, columns = ['b1','b2','b3','b4','b5', 'a1','a2','a3','a4','a5', 'bq1','bq2','bq3','bq4','bq5', 'aq1','aq2','aq3','aq4','aq5'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = generate_model(20)\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights = compute_class_weight('balanced', np.unique(y_train_val), y_train_val)\n",
    "# d_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 20\n",
    "# model_filename = \"model_resnet_walkforward_tcn_depth5.h5\"\n",
    "# hist_filename = \"hist_model_walkforward_tcn_depth5.csv\"\n",
    "# early_stop = keras.callbacks.EarlyStopping(\n",
    "#                 monitor='val_loss',\n",
    "#                 patience=4)\n",
    "# model_save_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "#                             filepath= model_filename,\n",
    "#                             save_best_only=True,\n",
    "#                             monitor='val_loss',\n",
    "#                             verbose=1)\n",
    "# reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "#                 monitor='val_loss',\n",
    "#                 factor=0.5,\n",
    "#                 patience=2,\n",
    "#                 verbose=1,\n",
    "#                 min_lr=0.0001)\n",
    "# callbacks = [reduce_lr, model_save_checkpoint, early_stop]\n",
    "# history = m.fit(\n",
    "#             X_train_val, y_train_val,\n",
    "#             epochs=EPOCHS,\n",
    "#             batch_size=128,\n",
    "#             callbacks=callbacks,\n",
    "#             class_weight = d_class_weights,\n",
    "#             validation_data = (X_test, y_test),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = m.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "    \n",
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# ax=sns.heatmap(cm, annot=True, xticklabels=['Down','Stable','Up'], yticklabels=['Down','Stable','Up'], cmap='Blues')\n",
    "# ax.set_ylim(3.0, 0)\n",
    "# plt.savefig('image_depth_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_true = y_test,y_pred = y_pred,  target_names=['Down', 'Stable', 'Up']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEPTH 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_val, y_train_val, X_test, y_test = generate_data(l2_snap, 4, columns=['b1','a1','bq1','aq1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = generate_model(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights = compute_class_weight('balanced', np.unique(y_train_val), y_train_val)\n",
    "# d_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 20\n",
    "# model_filename = \"model_resnet_walkforward_tcn.h5\"\n",
    "# hist_filename = \"hist_model_walkforward_tcn.csv\"\n",
    "# early_stop = keras.callbacks.EarlyStopping(\n",
    "#                 monitor='val_loss',\n",
    "#                 patience=4)\n",
    "# model_save_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "#                             filepath= model_filename,\n",
    "#                             save_best_only=True,\n",
    "#                             monitor='val_loss',\n",
    "#                             verbose=1)\n",
    "# reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "#                 monitor='val_loss',\n",
    "#                 factor=0.5,\n",
    "#                 patience=2,\n",
    "#                 verbose=1,\n",
    "#                 min_lr=0.0001)\n",
    "# callbacks = [reduce_lr, model_save_checkpoint, early_stop]\n",
    "# history = m.fit(\n",
    "#             X_train_val, y_train_val,\n",
    "#             epochs=EPOCHS,\n",
    "#             batch_size=128,\n",
    "#             callbacks=callbacks,\n",
    "#             class_weight = d_class_weights,\n",
    "#             validation_data = (X_test, y_test),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = m.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "    \n",
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# ax=sns.heatmap(cm, annot=True, xticklabels=['Down','Stable','Up'], yticklabels=['Down','Stable','Up'], cmap='Blues')\n",
    "# ax.set_ylim(3.0, 0)\n",
    "# plt.savefig('image_depth_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_true = y_test,y_pred = y_pred,  target_names=['Down', 'Stable', 'Up']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
