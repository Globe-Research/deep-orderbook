{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tcn in /opt/conda/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /opt/conda/lib/python3.7/site-packages (from keras-tcn) (1.18.5)\n",
      "Requirement already satisfied: keras==2.3.1 in /opt/conda/lib/python3.7/site-packages (from keras-tcn) (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (1.15.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (1.4.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (1.1.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1->keras-tcn) (5.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = './content/coinbase_btc_usd/coinbase/btc_usd/l2_snapshots/100ms/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_data(base_path):\n",
    "    \"\"\"Concatenate all the files in basepath keeping only the\n",
    "    columns specified by features.\n",
    "    \"\"\"\n",
    "    l2_snapshot = pd.DataFrame()\n",
    "    for i, x in enumerate(tqdm(sorted(os.listdir(base_path)))):\n",
    "        if base_path is None:\n",
    "            path = x\n",
    "        else:\n",
    "            path = base_path + x\n",
    "        df_hour = pd.read_parquet(path)\n",
    "        l2_snapshot = pd.concat([l2_snapshot, df_hour.dropna()])\n",
    "        \n",
    "    return l2_snapshot\n",
    "\n",
    "def extend_matrix(A, n):\n",
    "    \"\"\"Extend a matrix A by duplicating rows as specified by the list n.\"\"\"\n",
    "    n = n[1:]  # Do not duplicate rows for the first day\n",
    "    A = A[:-1]  # Do not duplicate the last day's row\n",
    "    A = np.repeat(A, repeats=n, axis=0)\n",
    "    return A\n",
    "\n",
    "def normalise_data_per_day(df):\n",
    "    df_mean = df.resample('D').mean()\n",
    "    df_var = df.resample('D').var()\n",
    "    \n",
    "    timestamps_per_day = np.unique(df.index.date, return_counts=True)[1]\n",
    "    mean_array = extend_matrix(df_mean.to_numpy(), timestamps_per_day)\n",
    "    var_array = extend_matrix(df_var.to_numpy(), timestamps_per_day)\n",
    "    \n",
    "    # Drop the rows of the first day\n",
    "    df = df[df.index.date != df.index[0].date()]\n",
    "    \n",
    "    df = (df - mean_array) / np.sqrt(var_array)\n",
    "\n",
    "    return df\n",
    "\n",
    "def balance_classes(y):\n",
    "    unique = np.unique(y, return_counts=True)\n",
    "\n",
    "    # Take smallest number as class size\n",
    "    class_size = np.min(unique[1])\n",
    "    class_size_index = np.argmin(unique[1])\n",
    "    timestamps = np.array([], dtype=int)\n",
    "    for i, category in enumerate(unique[0]):\n",
    "        if i == class_size_index:\n",
    "            continue\n",
    "        index = np.argwhere(y==category)\n",
    "        index = index.reshape(len(index))\n",
    "        random_timestamps = np.random.choice(index, (unique[1][i] - class_size), replace=False)\n",
    "        timestamps = np.concatenate((timestamps, random_timestamps), axis=None)\n",
    "        \n",
    "    return timestamps\n",
    "\n",
    "def generate_y(df_snapshot, k, T=100, D=40, best_ask='a1', best_bid='b1', alpha=10e-5):\n",
    "    \"\"\"Return X, y from the snapshot dataframe and the best ask/bid columns.\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['mid_price'] = (df_snapshot[best_ask].to_numpy()+df_snapshot[best_bid].to_numpy())/2\n",
    "\n",
    "    # Create columns delayed by -k to k-1\n",
    "    for i in range(-k, k):\n",
    "        df[i] = df['mid_price'].shift(periods=i)\n",
    "\n",
    "    # Drop first k-1 rows and last k rows\n",
    "    df.drop(range(0,k), axis=0, inplace=True)\n",
    "    df.drop(range(len(df_snapshot)-k,len(df_snapshot)), axis=0, inplace=True)\n",
    "    \n",
    "    # Compute mean of previous k and next k\n",
    "    df['m_b'] = df[range(0,k)].mean(axis=1)\n",
    "    df['m_a'] = df[range(-k,0)].mean(axis=1)\n",
    "    \n",
    "    # Compute label of increasing or decreasing\n",
    "    y_increase = np.where(df['m_b'] > df['m_a'] * (1+alpha), 1, 0)\n",
    "    y_decrease = np.where(df['m_b'] < df['m_a'] * (1-alpha), -1, 0)\n",
    "    y = y_increase + y_decrease\n",
    "\n",
    "    # 100 most recent limit orders used so ignore first 100 timesteps\n",
    "    y = y[T:]\n",
    "    y += 1\n",
    "\n",
    "    return y\n",
    "\n",
    "def generate_preX(df_snapshot, k):\n",
    "    preX = df_snapshot.to_numpy()[k:-k]\n",
    "\n",
    "    return preX\n",
    "\n",
    "def generate_X(preX, T=100, D=40):\n",
    "    # For each timestep create matrix of 100 most recent limit orders\n",
    "    X = np.array([preX[t:t+T] for t in range(len(preX)-T)], dtype='float32')\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [08:06<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# l2_snap = pd.read_csv('total.csv', index_col=0, infer_datetime_format=True)\n",
    "l2_snap = concat_data(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>...</th>\n",
       "      <th>aq41</th>\n",
       "      <th>aq42</th>\n",
       "      <th>aq43</th>\n",
       "      <th>aq44</th>\n",
       "      <th>aq45</th>\n",
       "      <th>aq46</th>\n",
       "      <th>aq47</th>\n",
       "      <th>aq48</th>\n",
       "      <th>aq49</th>\n",
       "      <th>aq50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.000</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.55</td>\n",
       "      <td>8719.50</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>8717.56</td>\n",
       "      <td>8716.06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>0.933419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.100</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.56</td>\n",
       "      <td>8719.55</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>8717.56</td>\n",
       "      <td>8716.06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>0.933419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.200</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.61</td>\n",
       "      <td>8719.56</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>8716.06</td>\n",
       "      <td>8716.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>8.200</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.300</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.61</td>\n",
       "      <td>8719.56</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8718.00</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>8716.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>8.200</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-12 00:00:00.400</th>\n",
       "      <td>8721.53</td>\n",
       "      <td>8720.59</td>\n",
       "      <td>8719.61</td>\n",
       "      <td>8719.56</td>\n",
       "      <td>8719.0</td>\n",
       "      <td>8718.99</td>\n",
       "      <td>8718.02</td>\n",
       "      <td>8718.00</td>\n",
       "      <td>8717.87</td>\n",
       "      <td>8717.85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>8.200</td>\n",
       "      <td>2.203</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.896321</td>\n",
       "      <td>0.766000</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>1.820000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              b1       b2       b3       b4      b5       b6  \\\n",
       "timestamp                                                                      \n",
       "2019-11-12 00:00:00.000  8721.53  8720.59  8719.55  8719.50  8719.0  8718.02   \n",
       "2019-11-12 00:00:00.100  8721.53  8720.59  8719.56  8719.55  8719.0  8718.02   \n",
       "2019-11-12 00:00:00.200  8721.53  8720.59  8719.61  8719.56  8719.0  8718.02   \n",
       "2019-11-12 00:00:00.300  8721.53  8720.59  8719.61  8719.56  8719.0  8718.02   \n",
       "2019-11-12 00:00:00.400  8721.53  8720.59  8719.61  8719.56  8719.0  8718.99   \n",
       "\n",
       "                              b7       b8       b9      b10  ...      aq41  \\\n",
       "timestamp                                                    ...             \n",
       "2019-11-12 00:00:00.000  8717.87  8717.85  8717.56  8716.06  ...  8.200000   \n",
       "2019-11-12 00:00:00.100  8717.87  8717.85  8717.56  8716.06  ...  8.200000   \n",
       "2019-11-12 00:00:00.200  8717.87  8717.85  8716.06  8716.00  ...  0.009135   \n",
       "2019-11-12 00:00:00.300  8718.00  8717.87  8717.85  8716.60  ...  0.009135   \n",
       "2019-11-12 00:00:00.400  8718.02  8718.00  8717.87  8717.85  ...  0.009135   \n",
       "\n",
       "                          aq42   aq43   aq44   aq45      aq46      aq47  \\\n",
       "timestamp                                                                 \n",
       "2019-11-12 00:00:00.000  2.203  2.600  0.005  2.500  0.896321  0.766000   \n",
       "2019-11-12 00:00:00.100  2.203  2.600  0.005  2.500  0.896321  0.766000   \n",
       "2019-11-12 00:00:00.200  8.200  2.203  2.600  0.005  2.500000  0.896321   \n",
       "2019-11-12 00:00:00.300  8.200  2.203  2.600  0.005  2.500000  0.896321   \n",
       "2019-11-12 00:00:00.400  8.200  2.203  2.600  0.005  2.500000  0.896321   \n",
       "\n",
       "                             aq48      aq49      aq50  \n",
       "timestamp                                              \n",
       "2019-11-12 00:00:00.000  0.001737  1.820000  0.933419  \n",
       "2019-11-12 00:00:00.100  0.001737  1.820000  0.933419  \n",
       "2019-11-12 00:00:00.200  0.766000  0.001737  1.820000  \n",
       "2019-11-12 00:00:00.300  0.766000  0.001737  1.820000  \n",
       "2019-11-12 00:00:00.400  0.766000  0.001737  1.820000  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_snap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>b10</th>\n",
       "      <th>...</th>\n",
       "      <th>aq41</th>\n",
       "      <th>aq42</th>\n",
       "      <th>aq43</th>\n",
       "      <th>aq44</th>\n",
       "      <th>aq45</th>\n",
       "      <th>aq46</th>\n",
       "      <th>aq47</th>\n",
       "      <th>aq48</th>\n",
       "      <th>aq49</th>\n",
       "      <th>aq50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:57.900</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:58.400</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:58.500</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:58.800</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-20 23:59:59.600</th>\n",
       "      <td>8082.96</td>\n",
       "      <td>8082.23</td>\n",
       "      <td>8081.65</td>\n",
       "      <td>8080.17</td>\n",
       "      <td>8079.74</td>\n",
       "      <td>8079.69</td>\n",
       "      <td>8079.19</td>\n",
       "      <td>8077.75</td>\n",
       "      <td>8077.6</td>\n",
       "      <td>8076.88</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.151276</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1.137</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.907671</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.74149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              b1       b2       b3       b4       b5       b6  \\\n",
       "timestamp                                                                       \n",
       "2019-11-20 23:59:57.900  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "2019-11-20 23:59:58.400  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "2019-11-20 23:59:58.500  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "2019-11-20 23:59:58.800  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "2019-11-20 23:59:59.600  8082.96  8082.23  8081.65  8080.17  8079.74  8079.69   \n",
       "\n",
       "                              b7       b8      b9      b10  ...   aq41  \\\n",
       "timestamp                                                   ...          \n",
       "2019-11-20 23:59:57.900  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "2019-11-20 23:59:58.400  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "2019-11-20 23:59:58.500  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "2019-11-20 23:59:58.800  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "2019-11-20 23:59:59.600  8079.19  8077.75  8077.6  8076.88  ...  0.005   \n",
       "\n",
       "                             aq42    aq43   aq44  aq45      aq46   aq47  \\\n",
       "timestamp                                                                 \n",
       "2019-11-20 23:59:57.900  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "2019-11-20 23:59:58.400  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "2019-11-20 23:59:58.500  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "2019-11-20 23:59:58.800  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "2019-11-20 23:59:59.600  0.151276  0.0058  1.137   0.6  0.907671  0.001   \n",
       "\n",
       "                          aq48  aq49     aq50  \n",
       "timestamp                                      \n",
       "2019-11-20 23:59:57.900  0.001   2.4  0.74149  \n",
       "2019-11-20 23:59:58.400  0.001   2.4  0.74149  \n",
       "2019-11-20 23:59:58.500  0.001   2.4  0.74149  \n",
       "2019-11-20 23:59:58.800  0.001   2.4  0.74149  \n",
       "2019-11-20 23:59:59.600  0.001   2.4  0.74149  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_snap.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Model and Depth Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Input, Model\n",
    "from tcn import TCN, tcn_full_summary\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(depth):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "\n",
    "        i = Input(batch_shape=(None, 100, depth))\n",
    "        o = TCN(return_sequences=False, use_skip_connections=True, dropout_rate=0.4, dilations=[1, 2, 4, 8, 16, 32, 64], use_batch_norm=True)(i)\n",
    "        o = Dense(3, activation='softmax')(o)\n",
    "        m = Model(inputs=[i], outputs=[o])\n",
    "        opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "        m.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(l2_snap, depth, columns, days_training, k=20):\n",
    "    l2_snap.index = pd.to_datetime(l2_snap.index)\n",
    "    l2_snap_dep2 = l2_snap.loc[:,columns]\n",
    "    l2_norm = normalise_data_per_day(l2_snap_dep2)\n",
    "    T = 100\n",
    "    D = depth\n",
    "    y = generate_y(l2_norm, T=T, D=D, best_ask='a1', best_bid = 'b1', alpha=0.002, k=k)\n",
    "    preX = generate_preX(l2_norm, k=k)\n",
    "    print(\"preX Shape: \", preX.shape)\n",
    "    X = generate_X(preX)\n",
    "    print(\"X shape, y shape: \", X.shape, y.shape)\n",
    "    print(\"Unique y's: \", np.unique(y))\n",
    "\n",
    "    del preX\n",
    "    gc.collect()\n",
    "\n",
    "    # First and last 20 are removed to create labels and then last 100 as previous 100 is required for input matrix\n",
    "    X_index = l2_norm[20:-120].index\n",
    "\n",
    "    # Number of data points for the last day\n",
    "    num_test = np.unique(X_index.day, return_counts=True)[1][-1]\n",
    "\n",
    "    # Days back to\n",
    "    num_start = np.unique(X_index.day, return_counts=True)[1][-1 - days_training:].sum()\n",
    "\n",
    "    # Split the data into the first days and the last day\n",
    "    X_train_val = X[-num_start:-num_test]\n",
    "    y_train_val = y[-num_start:-num_test]\n",
    "    X_test = X[-num_test:]\n",
    "    y_test = y[-num_test:]\n",
    "    del X\n",
    "    del y\n",
    "    gc.collect()\n",
    "    return X_train_val, y_train_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth:10\n",
    "\n",
    "\n",
    "#### Days 1, 2, 4, 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days Training now  1\n",
      "Level : 20\n",
      "preX Shape:  (4220032, 80)\n",
      "X shape, y shape:  (4219932, 100, 80) (4219932,)\n",
      "Unique y's:  [0 1 2]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[0 1 2], y=[1 1 1 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.7680 - accuracy: 0.4745\n",
      "Epoch 00001: val_loss improved from inf to 1.86127, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 154s 37ms/step - loss: 0.7680 - accuracy: 0.4745 - val_loss: 1.8613 - val_accuracy: 0.3361 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5466\n",
      "Epoch 00002: val_loss did not improve from 1.86127\n",
      "4128/4128 [==============================] - 149s 36ms/step - loss: 0.6893 - accuracy: 0.5467 - val_loss: 3.2840 - val_accuracy: 0.3794 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6726 - accuracy: 0.5665\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.86127\n",
      "4128/4128 [==============================] - 149s 36ms/step - loss: 0.6726 - accuracy: 0.5664 - val_loss: 4.5209 - val_accuracy: 0.4385 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6437 - accuracy: 0.5857\n",
      "Epoch 00004: val_loss did not improve from 1.86127\n",
      "4128/4128 [==============================] - 149s 36ms/step - loss: 0.6437 - accuracy: 0.5857 - val_loss: 4.7818 - val_accuracy: 0.4683 - lr: 0.0050\n",
      "Epoch 5/20\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.6354 - accuracy: 0.5912\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.86127\n",
      "4128/4128 [==============================] - 149s 36ms/step - loss: 0.6354 - accuracy: 0.5912 - val_loss: 6.4325 - val_accuracy: 0.5432 - lr: 0.0050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Down       0.26      0.37      0.30     61142\n",
      "      Stable       0.78      0.59      0.67    350499\n",
      "          Up       0.22      0.47      0.30     53759\n",
      "\n",
      "    accuracy                           0.54    465400\n",
      "   macro avg       0.42      0.47      0.42    465400\n",
      "weighted avg       0.64      0.54      0.58    465400\n",
      "\n",
      "ACCURACY:  0.5431027073485174\n",
      "Days Training now  1\n",
      "Level : 25\n",
      "preX Shape:  (4220032, 100)\n",
      "X shape, y shape:  (4219932, 100, 100) (4219932,)\n",
      "Unique y's:  [0 1 2]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[0 1 2], y=[1 1 1 ... 1 1 1] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 60 all-reduces with algorithm = nccl, num_packs = 1\n",
      "4128/4128 [==============================] - ETA: 0s - loss: 0.7642 - accuracy: 0.4729\n",
      "Epoch 00001: val_loss improved from inf to 2.34273, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 164s 40ms/step - loss: 0.7642 - accuracy: 0.4729 - val_loss: 2.3427 - val_accuracy: 0.3003 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6925 - accuracy: 0.5478\n",
      "Epoch 00002: val_loss improved from 2.34273 to 2.12364, saving model to model_resnet_walkforward_tcn_depth10.h5\n",
      "4128/4128 [==============================] - 160s 39ms/step - loss: 0.6925 - accuracy: 0.5478 - val_loss: 2.1236 - val_accuracy: 0.4103 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "4127/4128 [============================>.] - ETA: 0s - loss: 0.6705 - accuracy: 0.5681\n",
      "Epoch 00003: val_loss did not improve from 2.12364\n",
      "4128/4128 [==============================] - 161s 39ms/step - loss: 0.6705 - accuracy: 0.5681 - val_loss: 3.2380 - val_accuracy: 0.3609 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6576 - accuracy: 0.5786\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.12364\n",
      "4128/4128 [==============================] - 162s 39ms/step - loss: 0.6575 - accuracy: 0.5786 - val_loss: 5.2162 - val_accuracy: 0.4475 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "4126/4128 [============================>.] - ETA: 0s - loss: 0.6317 - accuracy: 0.5956"
     ]
    }
   ],
   "source": [
    "days_training = 1\n",
    "k = 20\n",
    "levels = {}\n",
    "for i in range(5):\n",
    "#     for level in [1, 2, 5, 10, 15, 20, 25, 30, 35, 40]:\n",
    "    for level in [20, 25, 30, 35, 40]:\n",
    "        columns = []\n",
    "        for i in range(level):\n",
    "            j = i + 1\n",
    "            columns.extend(['b{}'.format(j), 'bq{}'.format(j), 'a{}'.format(j), 'aq{}'.format(j)])\n",
    "        print('Days Training now ', days_training)\n",
    "        print('Level :', level)\n",
    "        depth = level * 4\n",
    "        X_train_val, y_train_val, X_test, y_test = generate_data(l2_snap, depth=depth, columns = columns, days_training=days_training, k=k)\n",
    "        m = generate_model(depth=depth)\n",
    "        from sklearn.utils.class_weight import compute_class_weight\n",
    "        class_weights = compute_class_weight('balanced', np.unique(y_train_val), y_train_val)\n",
    "        d_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        EPOCHS = 20\n",
    "        model_filename = \"model_resnet_walkforward_tcn_depth10.h5\"\n",
    "        hist_filename = \"hist_model_walkforward_tcn_depth10.csv\"\n",
    "        early_stop = keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        patience=4)\n",
    "        model_save_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "                                    filepath= model_filename,\n",
    "                                    save_best_only=True,\n",
    "                                    monitor='val_loss',\n",
    "                                    verbose=1)\n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "                        monitor='val_loss',\n",
    "                        factor=0.5,\n",
    "                        patience=2,\n",
    "                        verbose=1,\n",
    "                        min_lr=0.0001)\n",
    "        callbacks = [reduce_lr, model_save_checkpoint, early_stop]\n",
    "        history = m.fit(\n",
    "                    X_train_val, y_train_val,\n",
    "                    epochs=EPOCHS,\n",
    "                    batch_size=128,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight = d_class_weights,\n",
    "                    validation_data = (X_test, y_test),\n",
    "        )\n",
    "\n",
    "        y_pred = m.predict(X_test).argmax(axis=1)\n",
    "\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        sns.set()\n",
    "        cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        ax=sns.heatmap(cm, annot=True, xticklabels=['Down','Stable','Up'], yticklabels=['Down','Stable','Up'], cmap='Blues')\n",
    "        ax.set_ylim(3.0, 0)\n",
    "        plt.savefig('image_depth_10_{}_days_training'.format(days_training))\n",
    "\n",
    "        from sklearn.metrics import classification_report, accuracy_score\n",
    "        print(classification_report(y_true = y_test, y_pred = y_pred,  target_names=['Down', 'Stable', 'Up']))\n",
    "        accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\n",
    "        print('ACCURACY: ', accuracy)\n",
    "        if level in levels:\n",
    "            levels[level].append(accuracy)\n",
    "        else:\n",
    "            levels[level] = [accuracy]\n",
    "        del m\n",
    "        del X_train_val\n",
    "        del y_train_val\n",
    "        del X_test\n",
    "        del y_test\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(np.array(list(levels.keys()))/4)\n",
    "plt.plot(list(np.array(list(levels.keys()))/4), list(levels.values()))\n",
    "plt.ylabel('Walk-forward accuracy')\n",
    "plt.xlabel('Time Horizon s')\n",
    "plt.ylim([0, 0.7])\n",
    "# plt.savefig('walkforward_days_training')\n",
    "list(np.array(list(levels.keys()))/4)\n",
    "levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_val, y_train_val, X_test, y_test = generate_data(l2_snap, 20, columns = ['b1','b2','b3','b4','b5', 'a1','a2','a3','a4','a5', 'bq1','bq2','bq3','bq4','bq5', 'aq1','aq2','aq3','aq4','aq5'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = generate_model(20)\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights = compute_class_weight('balanced', np.unique(y_train_val), y_train_val)\n",
    "# d_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 20\n",
    "# model_filename = \"model_resnet_walkforward_tcn_depth5.h5\"\n",
    "# hist_filename = \"hist_model_walkforward_tcn_depth5.csv\"\n",
    "# early_stop = keras.callbacks.EarlyStopping(\n",
    "#                 monitor='val_loss',\n",
    "#                 patience=4)\n",
    "# model_save_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "#                             filepath= model_filename,\n",
    "#                             save_best_only=True,\n",
    "#                             monitor='val_loss',\n",
    "#                             verbose=1)\n",
    "# reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "#                 monitor='val_loss',\n",
    "#                 factor=0.5,\n",
    "#                 patience=2,\n",
    "#                 verbose=1,\n",
    "#                 min_lr=0.0001)\n",
    "# callbacks = [reduce_lr, model_save_checkpoint, early_stop]\n",
    "# history = m.fit(\n",
    "#             X_train_val, y_train_val,\n",
    "#             epochs=EPOCHS,\n",
    "#             batch_size=128,\n",
    "#             callbacks=callbacks,\n",
    "#             class_weight = d_class_weights,\n",
    "#             validation_data = (X_test, y_test),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = m.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "    \n",
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# ax=sns.heatmap(cm, annot=True, xticklabels=['Down','Stable','Up'], yticklabels=['Down','Stable','Up'], cmap='Blues')\n",
    "# ax.set_ylim(3.0, 0)\n",
    "# plt.savefig('image_depth_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_true = y_test,y_pred = y_pred,  target_names=['Down', 'Stable', 'Up']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEPTH 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_val, y_train_val, X_test, y_test = generate_data(l2_snap, 4, columns=['b1','a1','bq1','aq1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = generate_model(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights = compute_class_weight('balanced', np.unique(y_train_val), y_train_val)\n",
    "# d_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 20\n",
    "# model_filename = \"model_resnet_walkforward_tcn.h5\"\n",
    "# hist_filename = \"hist_model_walkforward_tcn.csv\"\n",
    "# early_stop = keras.callbacks.EarlyStopping(\n",
    "#                 monitor='val_loss',\n",
    "#                 patience=4)\n",
    "# model_save_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "#                             filepath= model_filename,\n",
    "#                             save_best_only=True,\n",
    "#                             monitor='val_loss',\n",
    "#                             verbose=1)\n",
    "# reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "#                 monitor='val_loss',\n",
    "#                 factor=0.5,\n",
    "#                 patience=2,\n",
    "#                 verbose=1,\n",
    "#                 min_lr=0.0001)\n",
    "# callbacks = [reduce_lr, model_save_checkpoint, early_stop]\n",
    "# history = m.fit(\n",
    "#             X_train_val, y_train_val,\n",
    "#             epochs=EPOCHS,\n",
    "#             batch_size=128,\n",
    "#             callbacks=callbacks,\n",
    "#             class_weight = d_class_weights,\n",
    "#             validation_data = (X_test, y_test),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = m.predict(X_test).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set()\n",
    "# cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "    \n",
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# ax=sns.heatmap(cm, annot=True, xticklabels=['Down','Stable','Up'], yticklabels=['Down','Stable','Up'], cmap='Blues')\n",
    "# ax.set_ylim(3.0, 0)\n",
    "# plt.savefig('image_depth_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_true = y_test,y_pred = y_pred,  target_names=['Down', 'Stable', 'Up']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
