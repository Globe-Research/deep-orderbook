{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoinBase.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkj26/deep-orderbook/blob/rakshit_jha/CoinBase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh_SnT3l9HZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!wget -O dataset.7z https://www.dropbox.com/sh/w3qagq2ze9noxon/AACE6f4nkBAJaJEc7Nbf2nhla/coinbase_btc_usd.7z?dl=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTd7ONy-IO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install pyunpack\n",
        "!pip install patool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4mqvERn9yr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyunpack import Archive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmfVVJx0-GMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Archive('/content/dataset.7z').extractall(\"/content\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rc5Q7S_b-c0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33uRritzLow_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_ = '/content/coinbase_btc_usd/coinbase/btc_usd/l2_snapshots/100ms/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io0T4wiBJ5xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l2_snapshot = pd.read_parquet('/content/coinbase_btc_usd/coinbase/btc_usd/l2_snapshots/100ms/coinbase_btc_usd_l2_book_snapshots_depth50_2019_11_12_0000_0100.parquet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3x4QWVDeo3-",
        "colab_type": "code",
        "outputId": "9846a6b1-28aa-4b0e-baf9-521260a22567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "l2_snapshot.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>b1</th>\n",
              "      <th>b2</th>\n",
              "      <th>b3</th>\n",
              "      <th>b4</th>\n",
              "      <th>b5</th>\n",
              "      <th>b6</th>\n",
              "      <th>b7</th>\n",
              "      <th>b8</th>\n",
              "      <th>b9</th>\n",
              "      <th>b10</th>\n",
              "      <th>b11</th>\n",
              "      <th>b12</th>\n",
              "      <th>b13</th>\n",
              "      <th>b14</th>\n",
              "      <th>b15</th>\n",
              "      <th>b16</th>\n",
              "      <th>b17</th>\n",
              "      <th>b18</th>\n",
              "      <th>b19</th>\n",
              "      <th>b20</th>\n",
              "      <th>b21</th>\n",
              "      <th>b22</th>\n",
              "      <th>b23</th>\n",
              "      <th>b24</th>\n",
              "      <th>b25</th>\n",
              "      <th>b26</th>\n",
              "      <th>b27</th>\n",
              "      <th>b28</th>\n",
              "      <th>b29</th>\n",
              "      <th>b30</th>\n",
              "      <th>b31</th>\n",
              "      <th>b32</th>\n",
              "      <th>b33</th>\n",
              "      <th>b34</th>\n",
              "      <th>b35</th>\n",
              "      <th>b36</th>\n",
              "      <th>b37</th>\n",
              "      <th>b38</th>\n",
              "      <th>b39</th>\n",
              "      <th>b40</th>\n",
              "      <th>...</th>\n",
              "      <th>aq11</th>\n",
              "      <th>aq12</th>\n",
              "      <th>aq13</th>\n",
              "      <th>aq14</th>\n",
              "      <th>aq15</th>\n",
              "      <th>aq16</th>\n",
              "      <th>aq17</th>\n",
              "      <th>aq18</th>\n",
              "      <th>aq19</th>\n",
              "      <th>aq20</th>\n",
              "      <th>aq21</th>\n",
              "      <th>aq22</th>\n",
              "      <th>aq23</th>\n",
              "      <th>aq24</th>\n",
              "      <th>aq25</th>\n",
              "      <th>aq26</th>\n",
              "      <th>aq27</th>\n",
              "      <th>aq28</th>\n",
              "      <th>aq29</th>\n",
              "      <th>aq30</th>\n",
              "      <th>aq31</th>\n",
              "      <th>aq32</th>\n",
              "      <th>aq33</th>\n",
              "      <th>aq34</th>\n",
              "      <th>aq35</th>\n",
              "      <th>aq36</th>\n",
              "      <th>aq37</th>\n",
              "      <th>aq38</th>\n",
              "      <th>aq39</th>\n",
              "      <th>aq40</th>\n",
              "      <th>aq41</th>\n",
              "      <th>aq42</th>\n",
              "      <th>aq43</th>\n",
              "      <th>aq44</th>\n",
              "      <th>aq45</th>\n",
              "      <th>aq46</th>\n",
              "      <th>aq47</th>\n",
              "      <th>aq48</th>\n",
              "      <th>aq49</th>\n",
              "      <th>aq50</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2019-11-12 00:00:00.000</th>\n",
              "      <td>8721.53</td>\n",
              "      <td>8720.59</td>\n",
              "      <td>8719.55</td>\n",
              "      <td>8719.50</td>\n",
              "      <td>8719.0</td>\n",
              "      <td>8718.02</td>\n",
              "      <td>8717.87</td>\n",
              "      <td>8717.85</td>\n",
              "      <td>8717.56</td>\n",
              "      <td>8716.06</td>\n",
              "      <td>8716.00</td>\n",
              "      <td>8715.29</td>\n",
              "      <td>8715.00</td>\n",
              "      <td>8714.97</td>\n",
              "      <td>8714.61</td>\n",
              "      <td>8714.20</td>\n",
              "      <td>8713.93</td>\n",
              "      <td>8712.43</td>\n",
              "      <td>8712.41</td>\n",
              "      <td>8712.09</td>\n",
              "      <td>8710.56</td>\n",
              "      <td>8710.55</td>\n",
              "      <td>8710.00</td>\n",
              "      <td>8708.91</td>\n",
              "      <td>8708.69</td>\n",
              "      <td>8708.68</td>\n",
              "      <td>8708.19</td>\n",
              "      <td>8708.13</td>\n",
              "      <td>8707.99</td>\n",
              "      <td>8705.89</td>\n",
              "      <td>8705.21</td>\n",
              "      <td>8705.00</td>\n",
              "      <td>8704.09</td>\n",
              "      <td>8704.08</td>\n",
              "      <td>8703.0</td>\n",
              "      <td>8702.34</td>\n",
              "      <td>8701.50</td>\n",
              "      <td>8700.99</td>\n",
              "      <td>8700.60</td>\n",
              "      <td>8700.36</td>\n",
              "      <td>...</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.630734</td>\n",
              "      <td>5.999000</td>\n",
              "      <td>1.116316</td>\n",
              "      <td>0.00224</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.001889</td>\n",
              "      <td>4.100000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.060263</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.4099</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.68713</td>\n",
              "      <td>0.25000</td>\n",
              "      <td>1.141294</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>0.092358</td>\n",
              "      <td>0.688130</td>\n",
              "      <td>0.67100</td>\n",
              "      <td>0.2335</td>\n",
              "      <td>0.689853</td>\n",
              "      <td>0.747313</td>\n",
              "      <td>1.640000</td>\n",
              "      <td>1.042177</td>\n",
              "      <td>0.009135</td>\n",
              "      <td>8.200000</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.600</td>\n",
              "      <td>0.005</td>\n",
              "      <td>2.500</td>\n",
              "      <td>0.896321</td>\n",
              "      <td>0.766000</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>1.820000</td>\n",
              "      <td>0.933419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-12 00:00:00.100</th>\n",
              "      <td>8721.53</td>\n",
              "      <td>8720.59</td>\n",
              "      <td>8719.56</td>\n",
              "      <td>8719.55</td>\n",
              "      <td>8719.0</td>\n",
              "      <td>8718.02</td>\n",
              "      <td>8717.87</td>\n",
              "      <td>8717.85</td>\n",
              "      <td>8717.56</td>\n",
              "      <td>8716.06</td>\n",
              "      <td>8716.00</td>\n",
              "      <td>8715.29</td>\n",
              "      <td>8715.00</td>\n",
              "      <td>8714.97</td>\n",
              "      <td>8714.61</td>\n",
              "      <td>8714.20</td>\n",
              "      <td>8713.93</td>\n",
              "      <td>8712.43</td>\n",
              "      <td>8712.41</td>\n",
              "      <td>8712.09</td>\n",
              "      <td>8710.56</td>\n",
              "      <td>8710.55</td>\n",
              "      <td>8710.00</td>\n",
              "      <td>8708.91</td>\n",
              "      <td>8708.69</td>\n",
              "      <td>8708.68</td>\n",
              "      <td>8708.19</td>\n",
              "      <td>8708.13</td>\n",
              "      <td>8707.99</td>\n",
              "      <td>8705.89</td>\n",
              "      <td>8705.21</td>\n",
              "      <td>8705.00</td>\n",
              "      <td>8704.09</td>\n",
              "      <td>8704.08</td>\n",
              "      <td>8703.0</td>\n",
              "      <td>8702.34</td>\n",
              "      <td>8701.50</td>\n",
              "      <td>8700.99</td>\n",
              "      <td>8700.60</td>\n",
              "      <td>8700.36</td>\n",
              "      <td>...</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.630734</td>\n",
              "      <td>5.999000</td>\n",
              "      <td>1.116316</td>\n",
              "      <td>0.00224</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.001889</td>\n",
              "      <td>4.100000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.060263</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.4099</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.68713</td>\n",
              "      <td>0.25000</td>\n",
              "      <td>1.141294</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>0.092358</td>\n",
              "      <td>0.688130</td>\n",
              "      <td>0.67100</td>\n",
              "      <td>0.2335</td>\n",
              "      <td>0.689853</td>\n",
              "      <td>0.747313</td>\n",
              "      <td>1.640000</td>\n",
              "      <td>1.042177</td>\n",
              "      <td>0.009135</td>\n",
              "      <td>8.200000</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.600</td>\n",
              "      <td>0.005</td>\n",
              "      <td>2.500</td>\n",
              "      <td>0.896321</td>\n",
              "      <td>0.766000</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>1.820000</td>\n",
              "      <td>0.933419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-12 00:00:00.200</th>\n",
              "      <td>8721.53</td>\n",
              "      <td>8720.59</td>\n",
              "      <td>8719.61</td>\n",
              "      <td>8719.56</td>\n",
              "      <td>8719.0</td>\n",
              "      <td>8718.02</td>\n",
              "      <td>8717.87</td>\n",
              "      <td>8717.85</td>\n",
              "      <td>8716.06</td>\n",
              "      <td>8716.00</td>\n",
              "      <td>8715.29</td>\n",
              "      <td>8715.00</td>\n",
              "      <td>8714.61</td>\n",
              "      <td>8714.20</td>\n",
              "      <td>8713.93</td>\n",
              "      <td>8712.43</td>\n",
              "      <td>8712.41</td>\n",
              "      <td>8712.09</td>\n",
              "      <td>8710.56</td>\n",
              "      <td>8710.55</td>\n",
              "      <td>8710.00</td>\n",
              "      <td>8708.91</td>\n",
              "      <td>8708.69</td>\n",
              "      <td>8708.68</td>\n",
              "      <td>8708.19</td>\n",
              "      <td>8708.13</td>\n",
              "      <td>8707.99</td>\n",
              "      <td>8705.89</td>\n",
              "      <td>8705.21</td>\n",
              "      <td>8705.00</td>\n",
              "      <td>8704.09</td>\n",
              "      <td>8704.08</td>\n",
              "      <td>8703.00</td>\n",
              "      <td>8702.34</td>\n",
              "      <td>8701.5</td>\n",
              "      <td>8700.99</td>\n",
              "      <td>8700.60</td>\n",
              "      <td>8700.36</td>\n",
              "      <td>8700.02</td>\n",
              "      <td>8700.00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.630734</td>\n",
              "      <td>5.999000</td>\n",
              "      <td>0.03030</td>\n",
              "      <td>0.00224</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.001889</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.060263</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.4099</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.68713</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.141294</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>0.092358</td>\n",
              "      <td>0.68813</td>\n",
              "      <td>0.6710</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.689853</td>\n",
              "      <td>0.747313</td>\n",
              "      <td>1.640000</td>\n",
              "      <td>1.042177</td>\n",
              "      <td>0.009135</td>\n",
              "      <td>8.200</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.600</td>\n",
              "      <td>0.005</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>0.896321</td>\n",
              "      <td>0.766000</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>1.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-12 00:00:00.300</th>\n",
              "      <td>8721.53</td>\n",
              "      <td>8720.59</td>\n",
              "      <td>8719.61</td>\n",
              "      <td>8719.56</td>\n",
              "      <td>8719.0</td>\n",
              "      <td>8718.02</td>\n",
              "      <td>8718.00</td>\n",
              "      <td>8717.87</td>\n",
              "      <td>8717.85</td>\n",
              "      <td>8716.60</td>\n",
              "      <td>8716.06</td>\n",
              "      <td>8716.00</td>\n",
              "      <td>8715.29</td>\n",
              "      <td>8715.00</td>\n",
              "      <td>8714.61</td>\n",
              "      <td>8714.20</td>\n",
              "      <td>8713.93</td>\n",
              "      <td>8713.10</td>\n",
              "      <td>8712.41</td>\n",
              "      <td>8712.09</td>\n",
              "      <td>8710.56</td>\n",
              "      <td>8710.55</td>\n",
              "      <td>8710.26</td>\n",
              "      <td>8710.00</td>\n",
              "      <td>8708.91</td>\n",
              "      <td>8708.69</td>\n",
              "      <td>8708.19</td>\n",
              "      <td>8708.13</td>\n",
              "      <td>8707.99</td>\n",
              "      <td>8705.89</td>\n",
              "      <td>8705.00</td>\n",
              "      <td>8704.09</td>\n",
              "      <td>8704.08</td>\n",
              "      <td>8703.00</td>\n",
              "      <td>8701.5</td>\n",
              "      <td>8700.99</td>\n",
              "      <td>8700.60</td>\n",
              "      <td>8700.36</td>\n",
              "      <td>8700.02</td>\n",
              "      <td>8700.00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.630734</td>\n",
              "      <td>5.999000</td>\n",
              "      <td>0.03030</td>\n",
              "      <td>0.00224</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.001889</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.060263</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.4099</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.68713</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.141294</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>0.092358</td>\n",
              "      <td>0.68813</td>\n",
              "      <td>0.6710</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.689853</td>\n",
              "      <td>0.747313</td>\n",
              "      <td>1.640000</td>\n",
              "      <td>1.042177</td>\n",
              "      <td>0.009135</td>\n",
              "      <td>8.200</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.600</td>\n",
              "      <td>0.005</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>0.896321</td>\n",
              "      <td>0.766000</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>1.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-11-12 00:00:00.400</th>\n",
              "      <td>8721.53</td>\n",
              "      <td>8720.59</td>\n",
              "      <td>8719.61</td>\n",
              "      <td>8719.56</td>\n",
              "      <td>8719.0</td>\n",
              "      <td>8718.99</td>\n",
              "      <td>8718.02</td>\n",
              "      <td>8718.00</td>\n",
              "      <td>8717.87</td>\n",
              "      <td>8717.85</td>\n",
              "      <td>8716.60</td>\n",
              "      <td>8716.06</td>\n",
              "      <td>8716.00</td>\n",
              "      <td>8715.29</td>\n",
              "      <td>8715.00</td>\n",
              "      <td>8714.61</td>\n",
              "      <td>8714.20</td>\n",
              "      <td>8713.93</td>\n",
              "      <td>8713.10</td>\n",
              "      <td>8712.41</td>\n",
              "      <td>8712.09</td>\n",
              "      <td>8710.56</td>\n",
              "      <td>8710.55</td>\n",
              "      <td>8710.26</td>\n",
              "      <td>8710.00</td>\n",
              "      <td>8708.91</td>\n",
              "      <td>8708.69</td>\n",
              "      <td>8708.19</td>\n",
              "      <td>8708.13</td>\n",
              "      <td>8707.99</td>\n",
              "      <td>8705.89</td>\n",
              "      <td>8705.00</td>\n",
              "      <td>8704.09</td>\n",
              "      <td>8704.08</td>\n",
              "      <td>8703.0</td>\n",
              "      <td>8701.50</td>\n",
              "      <td>8700.99</td>\n",
              "      <td>8700.60</td>\n",
              "      <td>8700.36</td>\n",
              "      <td>8700.02</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.630734</td>\n",
              "      <td>5.999000</td>\n",
              "      <td>0.03030</td>\n",
              "      <td>0.00224</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>0.001889</td>\n",
              "      <td>4.1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.060263</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.4099</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.68713</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.141294</td>\n",
              "      <td>3.500000</td>\n",
              "      <td>0.092358</td>\n",
              "      <td>0.68813</td>\n",
              "      <td>0.6710</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.689853</td>\n",
              "      <td>0.747313</td>\n",
              "      <td>1.640000</td>\n",
              "      <td>1.042177</td>\n",
              "      <td>0.009135</td>\n",
              "      <td>8.200</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.600</td>\n",
              "      <td>0.005</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>0.896321</td>\n",
              "      <td>0.766000</td>\n",
              "      <td>0.001737</td>\n",
              "      <td>1.820000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              b1       b2  ...      aq49      aq50\n",
              "timestamp                                  ...                    \n",
              "2019-11-12 00:00:00.000  8721.53  8720.59  ...  1.820000  0.933419\n",
              "2019-11-12 00:00:00.100  8721.53  8720.59  ...  1.820000  0.933419\n",
              "2019-11-12 00:00:00.200  8721.53  8720.59  ...  0.001737  1.820000\n",
              "2019-11-12 00:00:00.300  8721.53  8720.59  ...  0.001737  1.820000\n",
              "2019-11-12 00:00:00.400  8721.53  8720.59  ...  0.001737  1.820000\n",
              "\n",
              "[5 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ED9NkndJkCK",
        "colab_type": "code",
        "outputId": "7eae8315-1311-4de5-d65a-9ec6fae86b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "i = 0\n",
        "for x in os.listdir(path_):\n",
        "  #Remove the if part to load the full dataset\n",
        "  if i > 10:\n",
        "    break\n",
        "  else:\n",
        "    if i == 0:\n",
        "      l2_snapshot = pd.read_parquet('/content/coinbase_btc_usd/coinbase/btc_usd/l2_snapshots/100ms/coinbase_btc_usd_l2_book_snapshots_depth50_2019_11_12_0000_0100.parquet')\n",
        "    else:\n",
        "      temp = pd.read_parquet(path_+x)\n",
        "      l2_snapshot_ = l2_snapshot.copy()\n",
        "      l2_snapshot = pd.concat([l2_snapshot_, temp])\n",
        "      del temp\n",
        "      del l2_snapshot_\n",
        "      gc.collect()\n",
        "    i = i+1\n",
        "print('Memory Usage: {} MB'.format(l2_snapshot.memory_usage().sum()/(1024**2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "Memory Usage: 607.2401504516602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rzKlDvrzjyK",
        "colab_type": "code",
        "outputId": "f4d0b8d4-f0a8-4821-9d6b-f28b64c61f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Before dropping null values: {}'.format(l2_snapshot.shape))\n",
        "l2_snapshot.dropna(inplace=True)\n",
        "print('After dropping null values: {}'.format(l2_snapshot.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(395981, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdEO-8eBHM4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to generate the labels for learning\n",
        "def generate_features_labels(df,ask, bid,k=20, alpha = 10e-6):\n",
        "  df_ = df.copy()\n",
        "  df_['mid_price'] = (df_[ask]+df_[bid])/2\n",
        "  df_['target'] = 1\n",
        "  index = df_.columns.get_loc('mid_price')\n",
        "  target_index = df_.columns.get_loc('target')\n",
        "  shape = df_.shape[0]\n",
        "  for i in tqdm(range(k,shape-k)):\n",
        "    m_b = np.mean(df_.iloc[(i-k):i, index].values)\n",
        "    m_a = np.mean(df_.iloc[i+1:(i+k+1), index].values)\n",
        "\n",
        "    if (m_b > m_a*(1+alpha)):\n",
        "      df_.iloc[i,target_index] = 2\n",
        "    if (m_b < m_a*(1-alpha)):\n",
        "      df_.iloc[i,target_index] = 0\n",
        "\n",
        "  y = df_.iloc[k:shape-k, target_index].values\n",
        "  X = df.iloc[k:shape-k,:].values\n",
        "  #Free up memory by deleting not required dataframes.\n",
        "  del df_\n",
        "  del df\n",
        "  gc.collect()\n",
        "  return X,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDbvInqagtRk",
        "colab_type": "code",
        "outputId": "ffbbb26d-336c-41e8-c947-517fd6e1dbde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "X,y = generate_features_labels(l2_snapshot, ask='a1', bid = 'b1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-98d986a922d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_features_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_snapshot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'b1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'generate_features_labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy_ivAgHDAqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "T = 100\n",
        "D = 200\n",
        "N = len(X) - T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx7z59QgDRgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainPart = int(len(X)*0.7)  #(70% Data for training and 30% for testing)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X[:trainPart + T - 1])\n",
        "X = scaler.transform(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7D-o4gGFq5lD",
        "outputId": "85e8c5b8-806b-4905-f6ad-a73ab5272f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Create the template structure of training set\n",
        "X_train = np.zeros((trainPart, T, D))\n",
        "Y_train = np.zeros(trainPart)\n",
        "\n",
        "#Preparing the time series data using timestep of 100 and no of features = 200\n",
        "for t in range(trainPart):\n",
        "  X_train[t, :, :] = X[t:t+T]\n",
        "  Y_train[t] = y[t+T] \n",
        "\n",
        "# Reducing X_train datasize memory usage\n",
        "print('X_train - Before: {} GB'.format(X_train.nbytes/1024**3))\n",
        "X_train = X_train.astype('float16')\n",
        "print('X_train - After: {} GB'.format(X_train.nbytes/1024**3))\n",
        "\n",
        "# Reducing Y_train datasize memory usage\n",
        "print('Y_train - before: {} GB'.format(Y_train.nbytes/1024**3))\n",
        "Y_train = Y_train.astype('int16')\n",
        "print('Y_train - After: {} GB'.format(Y_train.nbytes/1024**3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26452.94189453125 1.3226470947265625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6FRnVV6EG1R",
        "colab_type": "code",
        "outputId": "cf03399a-8377-4699-da0c-272607160399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Create the template structure of test set\n",
        "X_test = np.zeros((N - trainPart, T, D))\n",
        "Y_test = np.zeros(N - trainPart)\n",
        "\n",
        "#Preparing the time series data using timestep of 100 and no of features = 200\n",
        "for k in range(N - trainPart):\n",
        "  t = k + trainPart\n",
        "  X_test[k, :, :] = X[t:t+T]\n",
        "  Y_test[k] = y[t+T]\n",
        "\n",
        "# Reducing X_test datasize memory usage\n",
        "print('X_test - Before {} GB'.format(X_test.nbytes/1024**3))\n",
        "X_test = X_test.astype('float16')\n",
        "print('X_test - After {} GB'.format(X_test.nbytes/1024**3))\n",
        "\n",
        "# Reducing Y_test datasize memory usage\n",
        "print('Y_test - Before: {} GB'.format(Y_test.nbytes/1024**3))\n",
        "Y_test = Y_test.astype('int16')\n",
        "print('Y_test - After: {} GB'.format(Y_test.nbytes/1024**3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before 11321.71630859375 MB\n",
            "After 2830.4290771484375 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EEo4XmdFYRv",
        "colab_type": "code",
        "outputId": "b1a3ce4b-6be8-4d99-cf93-c671cdfaaf89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Install TensorFlow\n",
        "# !pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x  # Colab only.\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x  # Colab only.`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AisEvFPTjuZ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv1D, Conv2D, Flatten, MaxPooling1D, MaxPooling2D, Dense, LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjeF7paIHkLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Converting X_train and X_test for 2D Convolution\n",
        "X_train = np.expand_dims(X_train, -1)\n",
        "X_test = np.expand_dims(X_test, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkZo5eXYmkDd",
        "colab_type": "code",
        "outputId": "17e49e76-4bf7-4982-dbd6-a401fce7eb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Model architecture is as from the report\n",
        "#T = 100 from the above step for timesteps and D = 200 (no. of features) \n",
        "i = Input(shape=X_train[0].shape)\n",
        "print(i)\n",
        "x = Conv2D(16, (4,D), activation=LeakyReLU(alpha=0.01))(i)\n",
        "print(x.shape)\n",
        "x = tf.keras.layers.Reshape(target_shape=(T-3,16))(x)\n",
        "print(x.shape)\n",
        "x = Conv1D(16, 4, activation=LeakyReLU(alpha=0.01))(x)\n",
        "x = MaxPooling1D(2)(x)\n",
        "x = Conv1D(32, 3, activation=LeakyReLU(alpha=0.01))(x)\n",
        "x = Conv1D(32, 3, activation=LeakyReLU(alpha=0.01))(x)\n",
        "x = MaxPooling1D(2)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(32, activation=LeakyReLU(alpha=0.01))(x)\n",
        "x = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(i,x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"input_1:0\", shape=(None, 100, 200, 1), dtype=float32)\n",
            "(None, 97, 1, 16)\n",
            "(None, 97, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iWtFhsNFUqr",
        "colab_type": "code",
        "outputId": "d01a29aa-f1de-4615-df38-75586eb327a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "#Printing Model Summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 100, 200, 1)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 97, 1, 16)         12816     \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 97, 16)            0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 94, 16)            1040      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 47, 16)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 45, 32)            1568      \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 43, 32)            3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 21, 32)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 672)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                21536     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 40,163\n",
            "Trainable params: 40,163\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxcsMebPpWS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "#Accuracy is not a great metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwRmHml2n42P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "def compute_class_weight(y):\n",
        "    classes = np.unique(y)\n",
        "    class_weight = compute_class_weight(\"balanced\", classes, y)\n",
        "    class_weights = dict(zip(classes, class_weight))\n",
        "    return class_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJMve9aBhUIw",
        "colab_type": "code",
        "outputId": "317f674b-da27-4cef-9580-3480e36e03bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "#computing class weights according to the percentage in training dataset\n",
        "class_weights = compute_class_weight(Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc85313a128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAD4CAYAAAAgs6s2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwab07y6Frhn",
        "colab_type": "code",
        "outputId": "272dd0b3-1850-4a52-889d-73e5a774cd55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "EPOCHS = 50\n",
        "BATCH_SIZE = 16\n",
        "r = model.fit(\n",
        "  X_train, Y_train,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_data=(X_test, Y_test),\n",
        "  class_weight = class_weights\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 173362 samples, validate on 74198 samples\n",
            "Epoch 1/50\n",
            "173362/173362 [==============================] - 168s 967us/sample - loss: 0.1457 - accuracy: 0.9306 - val_loss: 3.9394 - val_accuracy: 0.8041\n",
            "Epoch 2/50\n",
            "173362/173362 [==============================] - 164s 945us/sample - loss: 0.1443 - accuracy: 0.9284 - val_loss: 6.0285 - val_accuracy: 0.8263\n",
            "Epoch 3/50\n",
            "173362/173362 [==============================] - 163s 941us/sample - loss: 0.1321 - accuracy: 0.9351 - val_loss: 6.7715 - val_accuracy: 0.8327\n",
            "Epoch 4/50\n",
            "173362/173362 [==============================] - 166s 955us/sample - loss: 0.1318 - accuracy: 0.9359 - val_loss: 4.1682 - val_accuracy: 0.7835\n",
            "Epoch 5/50\n",
            "173362/173362 [==============================] - 165s 954us/sample - loss: 0.1280 - accuracy: 0.9386 - val_loss: 3.7676 - val_accuracy: 0.7809\n",
            "Epoch 6/50\n",
            "173362/173362 [==============================] - 165s 952us/sample - loss: 0.1151 - accuracy: 0.9453 - val_loss: 6.1028 - val_accuracy: 0.8367\n",
            "Epoch 7/50\n",
            "173362/173362 [==============================] - 163s 941us/sample - loss: 0.1185 - accuracy: 0.9447 - val_loss: 4.4522 - val_accuracy: 0.7688\n",
            "Epoch 8/50\n",
            "173362/173362 [==============================] - 167s 962us/sample - loss: 0.1025 - accuracy: 0.9500 - val_loss: 5.8868 - val_accuracy: 0.8475\n",
            "Epoch 9/50\n",
            "173362/173362 [==============================] - 163s 941us/sample - loss: 0.0933 - accuracy: 0.9538 - val_loss: 6.2162 - val_accuracy: 0.8342\n",
            "Epoch 10/50\n",
            "173362/173362 [==============================] - 165s 954us/sample - loss: 0.0986 - accuracy: 0.9514 - val_loss: 8.7974 - val_accuracy: 0.8624\n",
            "Epoch 11/50\n",
            "173362/173362 [==============================] - 169s 973us/sample - loss: 0.1081 - accuracy: 0.9490 - val_loss: 6.7938 - val_accuracy: 0.8327\n",
            "Epoch 12/50\n",
            "173362/173362 [==============================] - 168s 972us/sample - loss: 0.0964 - accuracy: 0.9536 - val_loss: 6.8921 - val_accuracy: 0.8242\n",
            "Epoch 13/50\n",
            "173362/173362 [==============================] - 166s 960us/sample - loss: 0.0911 - accuracy: 0.9552 - val_loss: 7.0952 - val_accuracy: 0.8334\n",
            "Epoch 14/50\n",
            "173362/173362 [==============================] - 167s 966us/sample - loss: 0.0833 - accuracy: 0.9566 - val_loss: 6.8688 - val_accuracy: 0.8075\n",
            "Epoch 15/50\n",
            "173362/173362 [==============================] - 168s 970us/sample - loss: 0.0917 - accuracy: 0.9541 - val_loss: 10.9260 - val_accuracy: 0.8504\n",
            "Epoch 16/50\n",
            "173362/173362 [==============================] - 168s 970us/sample - loss: 0.0877 - accuracy: 0.9589 - val_loss: 10.1090 - val_accuracy: 0.8641\n",
            "Epoch 17/50\n",
            "173362/173362 [==============================] - 171s 984us/sample - loss: 0.0847 - accuracy: 0.9579 - val_loss: 6.5370 - val_accuracy: 0.7600\n",
            "Epoch 18/50\n",
            "173362/173362 [==============================] - 170s 978us/sample - loss: 0.0877 - accuracy: 0.9561 - val_loss: 7.4623 - val_accuracy: 0.8076\n",
            "Epoch 19/50\n",
            "173362/173362 [==============================] - 169s 974us/sample - loss: 0.0848 - accuracy: 0.9583 - val_loss: 9.2325 - val_accuracy: 0.8520\n",
            "Epoch 20/50\n",
            "173362/173362 [==============================] - 169s 977us/sample - loss: 0.0819 - accuracy: 0.9592 - val_loss: 6.8327 - val_accuracy: 0.7472\n",
            "Epoch 21/50\n",
            "173362/173362 [==============================] - 171s 985us/sample - loss: 0.0907 - accuracy: 0.9554 - val_loss: 9.4762 - val_accuracy: 0.8124\n",
            "Epoch 22/50\n",
            "115488/173362 [==================>...........] - ETA: 48s - loss: 0.0722 - accuracy: 0.9626Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcf2wsdzSSoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.lineplot(x=range(1,EPOCHS+1),y=r.history['loss'])\n",
        "sns.lineplot(x=range(1,EPOCHS+1),y=r.history['val_loss'])\n",
        "plt.title('Model Cross Entropy Loss')\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Training', 'Testing'], loc='upper left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyGI6Al2n30C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_test = model.predict(X_test).argmax(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJugwyIJI_DC",
        "colab_type": "code",
        "outputId": "8421ed4e-e781-4c82-ecb6-173a98172a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        " model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9935645e-01, 6.4353470e-04, 5.4635085e-10],\n",
              "       [9.9902296e-01, 9.7708602e-04, 8.1950363e-10],\n",
              "       [9.9846858e-01, 1.5314259e-03, 1.1067330e-08],\n",
              "       ...,\n",
              "       [2.0005477e-03, 9.9799949e-01, 4.9733966e-08],\n",
              "       [9.9095714e-04, 9.9900901e-01, 5.0577714e-08],\n",
              "       [9.2293173e-03, 9.9077058e-01, 1.0115249e-07]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMhHGaX6i-oG",
        "colab_type": "code",
        "outputId": "2146a4d0-5980-4d29-ff3e-813d157354a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Plot confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_pred=p_test, y_true=Y_test)\n",
        "    \n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "ax=sns.heatmap(cm, annot=True, xticklabels=[0,1,2], yticklabels=[0,1,2], cmap='Blues')\n",
        "ax.set_ylim(3.0, 0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_tLLfJA6IT0",
        "colab_type": "code",
        "outputId": "f08c0c72-3377-4551-b399-d651a12ebfa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Print the classification report\n",
        "print(classification_report(y_true= Y_test, y_pred = p_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.17      0.13      3099\n",
            "           1       0.91      0.90      0.90     67169\n",
            "           2       0.10      0.07      0.08      3930\n",
            "\n",
            "    accuracy                           0.83     74198\n",
            "   macro avg       0.37      0.38      0.37     74198\n",
            "weighted avg       0.83      0.83      0.83     74198\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI1ofgzv6NiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}